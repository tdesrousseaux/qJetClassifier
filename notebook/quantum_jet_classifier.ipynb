{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Jet Classifier\n",
    "\n",
    "The aim of this project is to produce a jet classifier using a quantum graph neural network.\n",
    "\n",
    "This project is inspired of the following article: [*JEDI-net: a jet identification algorithm based on interaction networks*](https://arxiv.org/abs/1908.05318)\n",
    "\n",
    "This project was implemented on FPGAs with a simplified dataset (see [https://ipa.phys.ethz.ch/education/colloquium-ipa/current_semester.html](https://ipa.phys.ethz.ch/education/colloquium-ipa/current_semester.html)). This simplified version will be used as a reference for the current project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Circuit implementation\n",
    "\n",
    "The circuit takes a list of $N_p$ particles as an input. Each particle has $N_f$ features. \n",
    "\n",
    "A fully connected graph is first built the following way: \n",
    "- Each node corresponds to a particle and contains its features.\n",
    "- Each edge represents the concatenation of the features of the two corresponding nodes. \n",
    "\n",
    "A first circuit (for now a MLP) will transform the edges to a latent dimension (of dim $D_l$).\n",
    "\n",
    "The resulting graph structure is then embedded into a quantum circuit using angle encoding.\n",
    "\n",
    "The quantum circuit measurement gives the classification values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.12/site-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from tensorboardX) (23.1)\n",
      "Requirement already satisfied: protobuf>=3.20 in ./miniconda3/lib/python3.12/site-packages (from tensorboardX) (3.20.3)\n",
      "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.6.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.nn import Sequential\n",
    "import networkx as nx\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "try:\n",
    "  import tensorboardX\n",
    "except ModuleNotFoundError:\n",
    "  %pip install tensorboardX\n",
    "  import tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building the dataset\n",
    "\n",
    "The data come from https://zenodo.org/records/3602260\n",
    "\n",
    "It contains high-level features, but only restrained information are kept: for the N most energetic particles, $p_T, \\eta,\\phi$\n",
    "\n",
    "Then a graph structure is built using a fully connected graph from networkx. \n",
    "The nodes have the information of the particles. The edge represent the concatenation of the information of each pair of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/tdesrous/miniconda3/lib/python312.zip', '/data/tdesrous/miniconda3/lib/python3.12', '/data/tdesrous/miniconda3/lib/python3.12/lib-dynload', '', '/data/tdesrous/miniconda3/lib/python3.12/site-packages', '/data/tdesrous/qgraphs4feynman', '/data/tdesrous/qJetClassifier', '/tmp/tmpypt4yp06', '/data/tdesrous/qJetClassifier']\n"
     ]
    }
   ],
   "source": [
    "#This part of the program is adapted from https://github.com/bb511/deepsets_synth/blob/main/fast_deepsets/data/data.py\n",
    "\n",
    "# Handles the data importing and small preprocessing for the interaction network.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import wget\n",
    "import tarfile\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "# import tensorflow as tf\n",
    "qJetClassifier_path = os.path.abspath(os.path.join('..', 'tdesrous/qJetClassifier'))\n",
    "sys.path.append(qJetClassifier_path)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from qJetClassifier import fit_standardisation, apply_standardisation\n",
    "# from fast_deepsets.data import plots\n",
    "\n",
    "class HLS4MLData150(object):\n",
    "    \"\"\"Data class for importing and processing the jet data.\n",
    "\n",
    "    The raw data is available at https://zenodo.org/records/3602260.\n",
    "    See Moreno et. al. 2019 - JEDI-net: a jet identification algorithm for a full\n",
    "    description of this data set, section 3.\n",
    "\n",
    "    Args:\n",
    "        root: The root directory of the data. It should contain a 'raw' and 'processed'\n",
    "            folder with raw and processed data. Otherwise, these will be generated.\n",
    "        nconst: The number of constituents the jet data should be sampled down to.\n",
    "            The raw number of constituents is 150.\n",
    "        feats: Which feature selection scheme should be applied. 'ptetaphi' for getting\n",
    "            the transverse momentum, pseudo-rapidity, and azimuthal angle of each\n",
    "            cosntituents for every jet. Otherwise, 'all' gets all the features of\n",
    "            each constituents.\n",
    "        norm: What kind of normalisation to apply to the features of the data.\n",
    "            Currently implemented: minmax, robust, or standard.\n",
    "        train: Whether to import the training data (True) or validation data (False)\n",
    "            of this data set.\n",
    "        seed: If provided, shuffles the *constituents* in the data set with given seed.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        nconst: int,\n",
    "        feats: str,\n",
    "        norm: str,\n",
    "        train: bool,\n",
    "        kfolds: 0,\n",
    "        seed: int = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root = Path(root)\n",
    "        self.nconst = nconst\n",
    "        self.norm = norm\n",
    "        self.feats = feats\n",
    "        self.train = train\n",
    "        self.type = \"train\" if self.train else \"val\"\n",
    "        self.seed = seed\n",
    "        self.min_pt = 2\n",
    "        self.kfolds = kfolds\n",
    "\n",
    "        self.train_url = (\n",
    "            \"https://zenodo.org/records/3602260/files/hls4ml_LHCjet_150p_train.tar.gz\"\n",
    "        )\n",
    "        self.test_url = (\n",
    "            \"https://zenodo.org/records/3602260/files/hls4ml_LHCjet_150p_val.tar.gz\"\n",
    "        )\n",
    "\n",
    "        self.preproc_output_name = f\"{self.type}_{self.nconst}const.npy\"\n",
    "        self.proc_output_name = (\n",
    "            f\"{self.type}_{self.norm}_{self.nconst}const_{self.feats}.npy\"\n",
    "        )\n",
    "        os.umask(0)\n",
    "        self.data_file_dir = self._get_raw_data()\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self._get_processed_data()\n",
    "        self._kfold()\n",
    "\n",
    "        self.njets = self.x.shape[0]\n",
    "        self.nfeats = self.x.shape[-1]\n",
    "\n",
    "    def _get_raw_data(self) -> str:\n",
    "        \"\"\"Downloads and unzips the raw data if it does not exist.\n",
    "\n",
    "        This method checks the given root directory specified the init of the class.\n",
    "        This root directory should have a specific structure, with the raw data files in\n",
    "        a subfolder called \"raw\".\n",
    "        \"\"\"\n",
    "        os.umask(0)\n",
    "        if not self._check_raw_data_exists():\n",
    "            self._download_data()\n",
    "\n",
    "        return self.root / \"raw\" / self.type\n",
    "\n",
    "    def _check_raw_data_exists(self) -> bool:\n",
    "        \"\"\"Checks if the data exists in the given root dir or needs to be downloaded.\"\"\"\n",
    "        os.umask(0)\n",
    "        if self.root.is_dir():\n",
    "            raw_dir = self.root / \"raw\"\n",
    "            if raw_dir.is_dir():\n",
    "                data_dir = raw_dir / self.type\n",
    "                if data_dir.is_dir():\n",
    "                    if any(data_dir.iterdir()):\n",
    "                        return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _check_preprocessed_data_exists(self) -> bool:\n",
    "        \"\"\"Checks if the preprocessed data exisits or needs to be re-processed.\"\"\"\n",
    "        if self.root.is_dir():\n",
    "            preproc_folder = self.root / \"preprocessed\"\n",
    "            x_preproc_file = preproc_folder / f\"x_{self.preproc_output_name}\"\n",
    "            y_preproc_file = preproc_folder / f\"y_{self.preproc_output_name}\"\n",
    "\n",
    "            if x_preproc_file.is_file() and y_preproc_file.is_file():\n",
    "                self.x_preprocessed = np.load(x_preproc_file)\n",
    "                self.y_preprocessed = np.load(y_preproc_file)\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _check_processed_data_exists(self) -> bool:\n",
    "        \"\"\"Checks if the processed data exisits or needs to be re-processed.\"\"\"\n",
    "        if self.root.is_dir():\n",
    "            proc_folder = self.root / \"processed\"\n",
    "            x_proc_file = proc_folder / f\"x_{self.proc_output_name}\"\n",
    "            y_proc_file = proc_folder / f\"y_{self.proc_output_name}\"\n",
    "\n",
    "            if x_proc_file.is_file() and y_proc_file.is_file():\n",
    "                self.x = np.load(x_proc_file)\n",
    "                self.y = np.load(y_proc_file)\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _download_data(self):\n",
    "        \"\"\"Downloads the jet data if it does not already exist.\"\"\"\n",
    "        raw_dir = self.root / \"raw\"\n",
    "        os.umask(0)\n",
    "        if not raw_dir.is_dir():\n",
    "            os.makedirs(raw_dir)\n",
    "\n",
    "        if self.train:\n",
    "            data_file_path = wget.download(self.train_url, out=str(raw_dir))\n",
    "        else:\n",
    "            data_file_path = wget.download(self.test_url, out=str(raw_dir))\n",
    "\n",
    "        print(\"\")\n",
    "        data_tar = tarfile.open(data_file_path, \"r:gz\")\n",
    "        data_tar.extractall(str(raw_dir))\n",
    "        data_tar.close()\n",
    "        os.remove(data_file_path)\n",
    "\n",
    "    def _import_raw_data(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Imports the raw data files into a numpy array.\"\"\"\n",
    "        dfiles = list(file for file in self.data_file_dir.iterdir() if file.is_file())\n",
    "        data = h5py.File(dfiles[0])\n",
    "        self.x_raw = data[\"jetConstituentList\"]\n",
    "        self.y_raw = data[\"jets\"][:, -6:-1]\n",
    "\n",
    "        for file_path in dfiles[1:]:\n",
    "            data = h5py.File(file_path)\n",
    "            add_x_data = data[\"jetConstituentList\"]\n",
    "            add_y_data = data[\"jets\"][:, -6:-1]\n",
    "            self.x_raw = np.concatenate((self.x_raw, add_x_data), axis=0)\n",
    "            self.y_raw = np.concatenate((self.y_raw, add_y_data), axis=0)\n",
    "\n",
    "    def _preproc_raw_data(self):\n",
    "        \"\"\"Applies preprocessing to the raw data.\n",
    "\n",
    "        The raw data contains jets (samples), each comprising up to 150 particle\n",
    "        constituents. Every constituent has a feature called transverse momentum which\n",
    "        describes, loosely speaking, the energy of the particle. This method filters\n",
    "        out all jet constituents that have this transverse momentum below a given\n",
    "        minimum threshold. Additionally, the constituents in the raw data are\n",
    "        ordered in descending order of tranverse momentum value. The first n\n",
    "        constituents are taken for each jet, where n is a number between 1 and 150.\n",
    "        \"\"\"\n",
    "        self.x_preprocessed = np.copy(self.x_raw)\n",
    "        del self.x_raw\n",
    "        self.y_preprocessed = np.copy(self.y_raw)\n",
    "        del self.y_raw\n",
    "        self._cut_transverse_momentum()\n",
    "        self._restrict_nb_constituents()\n",
    "\n",
    "        preproc_dir = self.root / \"preprocessed\"\n",
    "        if not preproc_dir.is_dir():\n",
    "            os.makedirs(preproc_dir)\n",
    "        np.save(preproc_dir / f\"x_{self.preproc_output_name}\", self.x_preprocessed)\n",
    "        np.save(preproc_dir / f\"y_{self.preproc_output_name}\", self.y_preprocessed)\n",
    "\n",
    "    def _process_data(self):\n",
    "        \"\"\"Processes the already processed data.\n",
    "\n",
    "        Namely, certain features are selected for each constituent of each jet.\n",
    "        Furthermore, each feature is normalized, using a certain normalization scheme.\n",
    "        For example, minmax normalization.\n",
    "        \"\"\"\n",
    "        self.x = np.copy(self.x_preprocessed)\n",
    "        del self.x_preprocessed\n",
    "        self.y = np.copy(self.y_preprocessed)\n",
    "        del self.y_preprocessed\n",
    "\n",
    "        proc_folder = self.root / \"processed\"\n",
    "        self._get_features()\n",
    "        norm_params = self._get_normalisation_params()\n",
    "\n",
    "        self.x = apply_standardisation(self.norm, self.x, norm_params)\n",
    "        if self.seed and self.train:\n",
    "            self.shuffle_constituents(self.seed)\n",
    "        self._plot_data()\n",
    "\n",
    "        if not proc_folder.is_dir():\n",
    "            os.makedirs(proc_folder)\n",
    "        proc_folder = self.root / \"processed\"\n",
    "        np.save(proc_folder / f\"x_{self.proc_output_name}\", self.x)\n",
    "        np.save(proc_folder / f\"y_{self.proc_output_name}\", self.y)\n",
    "\n",
    "        # Free up memory after finishing the preprocessing.\n",
    "        del self.x\n",
    "        del self.y\n",
    "        self.x = np.load(proc_folder / f\"x_{self.proc_output_name}\")\n",
    "        self.y = np.load(proc_folder / f\"y_{self.proc_output_name}\")\n",
    "\n",
    "    def _get_normalisation_params(self) -> list[float]:\n",
    "        \"\"\"Computes the normalisation parameters on the training data.\n",
    "\n",
    "        For example, computes the mean and standard deviation of a feature on the\n",
    "        training data and then applies it to the validation data.\n",
    "        This is done such that no information on the validation data is used before\n",
    "        passing it through the machine learning algorithm.\n",
    "        \"\"\"\n",
    "        proc_folder = self.root / \"processed\"\n",
    "        if not self.train:\n",
    "            train_data_name = proc_output_name = (\n",
    "                f\"train_{self.norm}_{self.nconst}const_{self.feats}.npy\"\n",
    "            )\n",
    "            try:\n",
    "                x_data_train = np.load(proc_folder / f\"x_{train_data_name}\")\n",
    "            except OSError as e:\n",
    "                print(\"Process training data with same hyperparameters first!\")\n",
    "                print(\"Need for normalisation of the validation data.\")\n",
    "                exit(1)\n",
    "\n",
    "            return fit_standardisation(self.norm, self.x)\n",
    "\n",
    "        return fit_standardisation(self.norm, self.x)\n",
    "\n",
    "    def _get_processed_data(self):\n",
    "        \"\"\"Imports the processed data if it exists. If not, generates it.\"\"\"\n",
    "        if not self._check_processed_data_exists():\n",
    "            if not self._check_preprocessed_data_exists():\n",
    "                self._import_raw_data()\n",
    "                self._preproc_raw_data()\n",
    "\n",
    "            self._process_data()\n",
    "\n",
    "    def _plot_data(self):\n",
    "        \"\"\"Plots the normalised data.\"\"\"\n",
    "        print(\"Plotting data...\")\n",
    "        # plots_folder = self.root / f\"plots_{self.norm}_{self.nconst}const_{self.feats}\"\n",
    "        # if not plots_folder.is_dir():\n",
    "        #     os.makedirs(plots_folder)\n",
    "\n",
    "        # plots.constituent_number(plots_folder, self.x, self.type)\n",
    "        # plots.normalised_data(plots_folder, self.x, self.y, self.type, self.feats)\n",
    "\n",
    "    def _get_features(self) -> np.ndarray:\n",
    "        \"\"\"Choose what feature selection to employ on the data. Return shape.\"\"\"\n",
    "        switcher = {\n",
    "            \"ptetaphi\": lambda: self._select_features_ptetaphi(self.x),\n",
    "            \"allfeats\": lambda: self._select_features_all(self.x),\n",
    "        }\n",
    "\n",
    "        self.x = switcher.get(self.feats, lambda: None)()\n",
    "        if self.x is None:\n",
    "            raise TypeError(\"Feature selection name not valid!\")\n",
    "\n",
    "    def _select_features_ptetaphi(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Selects (pT, etarel, phirel) features from the numpy jet array.\"\"\"\n",
    "        return data[:, :, [5, 8, 11]]\n",
    "\n",
    "    def _select_features_all(self, data: np.ndarray):\n",
    "        \"\"\"Gets all the features from the numpy jet array.\n",
    "\n",
    "        The features in this kind of 'selection' are:'\n",
    "        (px, py, pz, E, Erel, pT, ptrel, eta, etarel, etarot, phi, phirel, phirot,\n",
    "        deltaR, cos(theta), cos(thetarel), pdgid)\n",
    "        \"\"\"\n",
    "        return data[:, :, :]\n",
    "\n",
    "    def _cut_transverse_momentum(self):\n",
    "        \"\"\"Remove constituents that are below a certain transverse momentum from jets.\n",
    "\n",
    "        If a jet has no constituents with a momentum above the given threshold, then\n",
    "        the whole jet is removed.\n",
    "        \"\"\"\n",
    "        boolean_mask = self.x_preprocessed[:, :, 5] > self.min_pt\n",
    "        structure_memory = boolean_mask.sum(axis=1)\n",
    "        self.x_preprocessed = np.split(\n",
    "            self.x_preprocessed[boolean_mask, :], np.cumsum(structure_memory)[:-1]\n",
    "        )\n",
    "        self.x_preprocessed = [\n",
    "            jet_const for jet_const in self.x_preprocessed if jet_const.size > 0\n",
    "        ]\n",
    "        self.y_preprocessed = self.y_preprocessed[structure_memory > 0]\n",
    "\n",
    "    def _restrict_nb_constituents(self) -> np.ndarray:\n",
    "        \"\"\"Force each jet to have an equal number of constituents.\n",
    "\n",
    "        If the jet has more constituents then the given number, the surplus is discarded.\n",
    "        If the jet has less than the given number, then the jet vector is padded with 0\n",
    "        values until its length reaches the given number.\n",
    "        \"\"\"\n",
    "        for jet in range(len(self.x_preprocessed)):\n",
    "            if self.x_preprocessed[jet].shape[0] >= self.nconst:\n",
    "                self.x_preprocessed[jet] = self.x_preprocessed[jet][: self.nconst, :]\n",
    "            else:\n",
    "                padding_length = self.nconst - self.x_preprocessed[jet].shape[0]\n",
    "                self.x_preprocessed[jet] = np.pad(\n",
    "                    self.x_preprocessed[jet], ((0, padding_length), (0, 0))\n",
    "                )\n",
    "        self.x_preprocessed = np.array(self.x_preprocessed)\n",
    "\n",
    "    def shuffle_constituents(self, seed: int):\n",
    "        \"\"\"Shuffles the constituents based on an array of seeds.\n",
    "\n",
    "        Each jet's constituents is shuffled with respect to a seed that is fixed.\n",
    "        This seed is different for each jet.\n",
    "        \"\"\"\n",
    "        print(\"Shuffling constituents...\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "        seeds = rng.integers(low=0, high=10000, size=self.x.shape[0])\n",
    "\n",
    "        for jet_idx, seed in enumerate(seeds):\n",
    "            shuffling = np.random.RandomState(seed=seed).permutation(self.x.shape[1])\n",
    "            self.x[jet_idx, :] = self.x[jet_idx, shuffling]\n",
    "\n",
    "        print(f\"Shuffling done!\")\n",
    "\n",
    "    def _kfold(self):\n",
    "        \"\"\"Creates a kfolded view of the data.\"\"\"\n",
    "        if self.kfolds <= 0 or not self.train:\n",
    "            return\n",
    "\n",
    "        # print(tcols.OKGREEN)\n",
    "        print(f\"Splitting the data into k={self.kfolds} kfolds.\")\n",
    "        # print(tcols.ENDC)\n",
    "\n",
    "        kfolder = sklearn.model_selection.StratifiedKFold(\n",
    "            n_splits=self.kfolds, shuffle=True\n",
    "        )\n",
    "        # Convert back from one-hot to class targets since sklearn function does not\n",
    "        # like one-hot targets.\n",
    "        self.kfolds = kfolder.split(self.x, np.argmax(self.y, axis=-1))\n",
    "\n",
    "    def show_details(self):\n",
    "        \"\"\"Prints some key details of the data set.\"\"\"\n",
    "        data_type = \"Training\" if self.train else \"Validation\"\n",
    "        print(f\"{data_type} data details:\")\n",
    "        print(f\"Dataset size: {self.x.shape[0]} jets\")\n",
    "        print(f\"Number of constituents: {self.x.shape[1]}\")\n",
    "        print(f\"Number of features: {self.x.shape[2]}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HLS4MLData150(\n",
    "    root=\"/data/tdesrous/qJetClassifier/data\",\n",
    "    nconst=6,\n",
    "    feats=\"ptetaphi\",\n",
    "    norm=\"standard\",\n",
    "    train=True,\n",
    "    kfolds=0,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HLS4MLData150(\n",
    "    root=\"/data/tdesrous/qJetClassifier/data_test\",\n",
    "    nconst=6,\n",
    "    feats=\"ptetaphi\",\n",
    "    norm=\"standard\",\n",
    "    train=False,\n",
    "    kfolds=0,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_file_path: str, y_file_path: str, n_elements=750):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x_file_path (string): Path to the file with the x data.\n",
    "            y_file_path (string): Path to the file with the y data.\n",
    "            n_elements (int): Number of elements to take from the dataset\n",
    "        \"\"\"\n",
    "        self.x_file_path = x_file_path\n",
    "        self.y_file_path = y_file_path\n",
    "        self.n_elements = n_elements\n",
    "        self.data = self.__convert_to_graph(self.__load_data(x_file_path, y_file_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset\n",
    "        \n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the item at the given index\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the item to return\n",
    "            \n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        graph = self.data[idx][0]\n",
    "        label = self.data[idx][1]\n",
    "        graph = from_networkx(graph)\n",
    "        graph.y = label\n",
    "        return graph\n",
    "    \n",
    "    def __load_data(self, x_file_path: str, y_file_path: str):\n",
    "        x = np.load(x_file_path)\n",
    "        # y = np.load(y_file_path).astype(np.int64)\n",
    "        y = np.load(y_file_path)\n",
    "        indices_list = np.random.choice(x.shape[0], self.n_elements, replace=False)\n",
    "        return ([[x[i], y[i]] for i in indices_list])\n",
    "\n",
    "\n",
    "    def __convert_to_graph(self, dataset):\n",
    "        \n",
    "        graph_dataset = []\n",
    "        for data_element in dataset:\n",
    "            G = nx.DiGraph()\n",
    "            x = data_element[0]\n",
    "            # print(type(x[0]))\n",
    "            y = data_element[1]\n",
    "            nb_particles = x.shape[0]\n",
    "            for i in range(nb_particles):\n",
    "                G.add_node(i, pTEtaPhi=x[i])\n",
    "                for j in range(i+1, nb_particles):\n",
    "                    G.add_edge(i, j, edge = [x[i], x[j]])\n",
    "\n",
    "            graph_dataset.append([G,y])\n",
    "        \n",
    "        return graph_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_file_path = \"/data/tdesrous/qJetClassifier/data/processed/x_train_standard_6const_ptetaphi.npy\"\n",
    "y_file_path = \"/data/tdesrous/qJetClassifier/data/processed/y_train_standard_6const_ptetaphi.npy\"\n",
    "\n",
    "train_dataset = JetDataset(x_file_path, y_file_path, 1000)\n",
    "# print(train_dataset[0].is_undirected())\n",
    "# print(train_dataset[0].edge_index)\n",
    "# print(len(train_dataset[0].edge))\n",
    "# print(train_dataset[0].edge)\n",
    "\n",
    "graph = train_dataset[0]\n",
    "# print(to_networkx(train_dataset[0][0], to_undirected=True, edge_attrs=['edge'], node_attrs=['pTEtaPhi']).edges(data=True))\n",
    "# graph = to_networkx(train_dataset[0][0], to_undirected=True, edge_attrs=['edge'], node_attrs=['pTEtaPhi'])\n",
    "# print([x[0] for x in graph.edges[(0,1)]['edge']])\n",
    "# print(graph)\n",
    "# print(graph.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_file_path = \"/data/tdesrous/qJetClassifier/data_test/processed/x_val_standard_6const_ptetaphi.npy\"\n",
    "y_test_file_path = \"/data/tdesrous/qJetClassifier/data_test/processed/y_val_standard_6const_ptetaphi.npy\"\n",
    "\n",
    "test_dataset = JetDataset(x_test_file_path, y_test_file_path, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quantum circuit\n",
    "\n",
    "The quantum circuit takes as an input a fully connected graph.\n",
    "\n",
    "It uses data uploading the following way:\n",
    "- The node information are encoded using $R_X$, $R_Y$ and $R_Z$ rotations gates (there are three parameters by node). (Other possibility: use only $R_X$ gates with a vector encoding)\n",
    "- The edge information are encoded using $R_{ZZ}$ rotation gates.\n",
    "\n",
    "All the gates are parametrized, using equivariant layers (regarding the node permutations). It means that the trainable parameters are the same for each layer. \n",
    "\n",
    "Then a equivariant ansatz is applied.\n",
    "\n",
    "This process is done $N_R$ times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import contextlib\n",
    "\n",
    "def R_X_layer(graph, parameters):\n",
    "    \"\"\"Encode the nodes of the graph using R_X gates, multiplied by trainable parameters\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        parameters (torch.Tensor): the trainable parameters for the equivariant layer\n",
    "    \"\"\"\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        qml.RX(torch.inner(parameters, torch.tensor(graph.nodes[node]['pTEtaPhi'])),\n",
    "               wires=node)\n",
    "        \n",
    "def ZZ_layer(graph, parameters):\n",
    "    \"\"\"Encode the edges of the graph using ZZ gates, multiplied by trainable parameters\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        parameters (torch.Tensor): the trainable parameters for the equivariant layer\n",
    "    \"\"\"\n",
    "\n",
    "    for edge in graph.edges:  #The order doesn't matter, the gates commute between each other\n",
    "        qml.IsingZZ(torch.inner(parameters, torch.tensor(graph.edges[edge]['edge'])),\n",
    "                    wires=edge[:2])\n",
    "\n",
    "\n",
    "def feature_map(graph, feature_parameters):\n",
    "    \"\"\"Encode the nodes and edges of the graph using R_X and ZZ gates, multiplied by trainable parameters\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        feature_parameters (torch.Tensor): the trainable parameters for the feature map\n",
    "    \"\"\"\n",
    "    nb_node_features = len(graph.nodes[0]['pTEtaPhi'])\n",
    "    nb_edge_features = len(graph.edges[0,1]['edge'])\n",
    "    nb_parameters = len(feature_parameters)\n",
    "\n",
    "    assert nb_parameters == nb_node_features + nb_edge_features, \"The number of feature parameters is not correct\"\n",
    "\n",
    "\n",
    "    R_X_layer(graph, feature_parameters[:nb_node_features])\n",
    "    ZZ_layer(graph, feature_parameters[nb_node_features:])\n",
    "\n",
    "def equivariant_ansatz(graph, parameters, nb_layers):\n",
    "    \"\"\"The trainable equivariant ansatz\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        parameters (torch.Tensor): the trainable parameters for the equivariant layer\n",
    "        nb_layers (int): the number of layers of the ansatz\n",
    "    \"\"\"\n",
    "\n",
    "    nb_wires = len(graph.nodes)\n",
    "    assert len(parameters) == 4 * nb_layers, \"The number of parameters of the ansatz is not correct\"\n",
    "\n",
    "    for layer in range(nb_layers):\n",
    "        for i in range(nb_wires):\n",
    "            qml.Rot(*parameters[4*layer:4*layer+3], wires=i)\n",
    "        for edge in graph.edges:\n",
    "            qml.IsingZZ(parameters[4*layer+3], wires=edge[:2])\n",
    "\n",
    "def reuploading_circuit(graph, parameters, nb_reuploading, nb_ansatz_layers):\n",
    "    \"\"\"A circuit that encodes the graph using the re-uploading strategy\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        parameters (torch.Tensor): the trainable parameters for the equivariant layer\n",
    "        nb_reuploading (int): the number of re-uploading steps\n",
    "        nb_ansatz_layers (int): the number of layers of the ansatz\n",
    "    \"\"\"\n",
    "\n",
    "    nb_parameters = len(parameters)\n",
    "    nb_parameters_per_layer = nb_parameters // nb_reuploading\n",
    "    assert nb_parameters % nb_reuploading == 0, \"The number of parameters is not divisible by the number of re-uploading steps\"\n",
    "\n",
    "    for i in graph.nodes:\n",
    "        qml.Hadamard(wires=i)\n",
    "\n",
    "    for layer in range(nb_reuploading):\n",
    "        layer_parameters = parameters[layer*nb_parameters_per_layer:(layer+1)*nb_parameters_per_layer]\n",
    "        ansatz_parameters = layer_parameters[:4*nb_ansatz_layers]\n",
    "        feature_parameters = layer_parameters[4*nb_ansatz_layers:]\n",
    "\n",
    "        feature_map(graph, feature_parameters)\n",
    "        equivariant_ansatz(graph, ansatz_parameters, nb_ansatz_layers)\n",
    "\n",
    "    N = len(graph.nodes)\n",
    "    probability = [qml.expval(qml.Z(wire)) for wire in range (N)]          #TO MODIFY\n",
    "    # probability = qml.expval(qml.PauliZ(0))\n",
    "    return probability\n",
    "\n",
    "class QLayer(nn.Module):\n",
    "    def __init__(self, qnode, nb_parameters):   \n",
    "        super(QLayer, self).__init__()\n",
    "        self.qnode = qnode\n",
    "        self.nb_parameters = nb_parameters\n",
    "        self.qnode_weights: Dict[str, torch.nn.Parameter] = {}\n",
    "        self._init_weights()\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        \"\"\"If the qnode is initialized, first check to see if the attribute is on the qnode.\"\"\"\n",
    "        if self._initialized:\n",
    "            with contextlib.suppress(AttributeError):\n",
    "                return getattr(self.qnode, item)\n",
    "\n",
    "        return super().__getattr__(item)\n",
    "\n",
    "    def __setattr__(self, item, val):\n",
    "        \"\"\"If the qnode is initialized and item is already a qnode property, update it on the qnode, else\n",
    "        just update the torch layer itself.\"\"\"\n",
    "        if self._initialized and item in self.qnode.__dict__:\n",
    "            setattr(self.qnode, item, val)\n",
    "        else:\n",
    "            super().__setattr__(item, val)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.qnode_weights[\"weights\"] = nn.Parameter(torch.randn(self.nb_parameters))\n",
    "        self.register_parameter(\"weights\", self.qnode_weights[\"weights\"])\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # print(self.qnode_weights.items())\n",
    "        kwargs = {\n",
    "            **{self.input_arg: graph},\n",
    "            **{arg: weight for arg, weight in self.qnode_weights.items()},\n",
    "        }\n",
    "        \n",
    "        return self.qnode(**kwargs)\n",
    "    \n",
    "    def __str__(self):\n",
    "        detail = \"<Quantum Torch Layer: func={}>\"\n",
    "        return detail.format(self.qnode.func.__name__)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "    _input_arg = \"inputs\"\n",
    "    _initialized = False\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_arg(self):\n",
    "        \"\"\"Name of the argument to be used as the input to the Torch layer. Set to ``\"inputs\"``.\"\"\"\n",
    "        return self._input_arg\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_input_argument(input_name: str = \"inputs\") -> None:\n",
    "        \"\"\"\n",
    "        Set the name of the input argument.\n",
    "\n",
    "        Args:\n",
    "            input_name (str): Name of the input argument\n",
    "        \"\"\"\n",
    "        QLayer._input_arg = input_name\n",
    "\n",
    "    def construct(self, args, kwargs):\n",
    "        \"\"\"Constructs the wrapped QNode on input data using the initialized weights.\n",
    "\n",
    "        This method was added to match the QNode interface. The provided args\n",
    "        must contain a single item, which is the input to the layer. The provided\n",
    "        kwargs is unused.\n",
    "\n",
    "        Args:\n",
    "            args (tuple): A tuple containing one entry that is the input to this layer\n",
    "            kwargs (dict): Unused\n",
    "        \"\"\"\n",
    "        x = args[0]\n",
    "        kwargs = {\n",
    "            self.input_arg: x,\n",
    "            **{arg: weight.data for arg, weight in self.qnode_weights.items()},\n",
    "        }\n",
    "        self.qnode.construct((), kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Neural Network\n",
    "\n",
    "The architecture is the following:\n",
    "- The NN takes the graph as an input\n",
    "- It processes the edges\n",
    "- The resulting graph is plugged in the quantum circuit\n",
    "- The expectation values of the circuit is used in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qJetClassifier(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = model_params['hidden_layer_size']\n",
    "        self.latent_space_size = model_params['latent_space_size']\n",
    "        self.nb_reuploading = model_params['nb_reuploading']\n",
    "        self.nb_ansatz_layers = model_params['nb_ansatz_layers']\n",
    "        self.qdevice = model_params['qdevice']\n",
    "        self.diff_method = model_params['diff_method']\n",
    "        self.nb_node_features = model_params['nb_node_features']\n",
    "\n",
    "        self.l1 = nn.Linear(self.nb_node_features, self.hidden_layer_size, dtype=torch.float64)\n",
    "        self.l2 = nn.Linear(self.hidden_layer_size, self.latent_space_size, dtype=torch.float64)\n",
    "        self.qnode = qml.QNode(self._quantum_classifier, self.qdevice, interface=\"torch\", diff_method=self.diff_method)\n",
    "        \n",
    "        # self.q_weight_shapes = {\"weights\": self.nb_reuploading*(self.nb_ansatz_layers*4+self.nb_node_features+self.latent_space_size)}\n",
    "        # self.qlayers = [qml.qnn.TorchLayer(self.qnode, self.q_weight_shapes) for i in range(5)]\n",
    "        self.qlayers = [QLayer(self.qnode, self.nb_reuploading*(self.nb_ansatz_layers*4+self.nb_node_features+self.latent_space_size)) for i in range(5)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        edge_attr = x.edge\n",
    "        edge_attr = edge_attr[:,0], edge_attr[:,1]\n",
    "        # print(edge_attr)\n",
    "        edge_attr = nn.functional.leaky_relu(0.5*(self.l1(edge_attr[0]))+self.l1(edge_attr[1]))\n",
    "\n",
    "        edge_attr = nn.functional.softmax(self.l2(edge_attr))\n",
    "\n",
    "        x.edge = edge_attr\n",
    "        # for edge in x.edge:\n",
    "            \n",
    "\n",
    "        #     edge_attr = edge_attr[:,0], edge_attr[:,1]\n",
    "            \n",
    "        #     assert len(x1) == self.nb_node_features, \"The number of node features is not correct\"\n",
    "\n",
    "        #     message = nn.functional.leaky_relu(0.5*(self.l1(torch.tensor(x1))+self.l1(torch.tensor(x2))))\n",
    "        #     message = nn.functional.softmax(self.l2(message))\n",
    "\n",
    "        #     x.edges[edge]['edge'] = message\n",
    "\n",
    "        # print(x)\n",
    "        list_graph = x.to_data_list()\n",
    "        # print(list_graph)\n",
    "        output = []\n",
    "        for x in list_graph:   #TODO: Is there a better way to do this?\n",
    "            probabilities = []\n",
    "\n",
    "            for i in range(5):\n",
    "                measurement = self.qlayers[i](x)\n",
    "                # print(\"measurement:\", measurement)\n",
    "                measurement = torch.stack(measurement)\n",
    "                # print(\"measurement2:\", measurement)\n",
    "                measurement_normalized = torch.sum(measurement)/measurement.shape[0] + 0.5\n",
    "                probabilities.append(measurement_normalized)\n",
    "            # print(\"probabilities:\", probabilities)\n",
    "            output.append(torch.stack(probabilities))\n",
    "        return torch.stack(output)\n",
    "\n",
    "    def _quantum_classifier(self, inputs, weights):\n",
    "        \"\"\"The quantum classifier\n",
    "\n",
    "        Args:\n",
    "            inputs (Networkx.Graph): a fully connected graph representing the jet\n",
    "            weights (torch.Tensor): the trainable parameters for the classifier\n",
    "            nb_reuploading (int): the number of re-uploading steps\n",
    "            nb_ansatz_layers (int): the number of layers of the ansatz\n",
    "        \"\"\"\n",
    "        # print(inputs)\n",
    "        inputs = to_networkx(inputs, to_undirected=True, edge_attrs=['edge'], node_attrs=['pTEtaPhi'])\n",
    "        # print(inputs)\n",
    "        probability = reuploading_circuit(inputs, weights, self.nb_reuploading, self.nb_ansatz_layers)\n",
    "        # print(probability)\n",
    "        \n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "qJetClassifier(\n",
      "  (l1): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (l2): Linear(in_features=12, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'hidden_layer_size': 12,\n",
    "    'latent_space_size': 6,\n",
    "    'nb_reuploading': 3,\n",
    "    'nb_ansatz_layers': 3,\n",
    "    'qdevice': qml.device('lightning.qubit', wires=6),\n",
    "    'diff_method': 'adjoint',\n",
    "    'nb_node_features': 3\n",
    "}\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = qJetClassifier(model_params).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_dataset) * 0.8)\n",
    "valid_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_set_size, valid_set_size], generator=seed)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitQJetClassifier(L.LightningModule):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # print(\"size:\",batch.batch_size)\n",
    "        x, y = batch, batch.y[:batch.batch_size]\n",
    "        # print(y)\n",
    "        y = torch.tensor(y)\n",
    "        # print(y)\n",
    "\n",
    "        # y = torch.from_numpy(y[0])\n",
    "        # print(y)\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch, batch.y[:batch.batch_size]\n",
    "        y = torch.tensor(y)\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch, batch.y[:batch.batch_size]\n",
    "        y = torch.tensor(y)\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | qJetClassifier   | 126    | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "126       Trainable params\n",
      "0         Non-trainable params\n",
      "126       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a16feb284e455eaf741381cf6478eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160691/2837689712.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  edge_attr = nn.functional.softmax(self.l2(edge_attr))\n",
      "/data/tdesrous/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "qClassifier = LitQJetClassifier(model, loss_fn, optimizer)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10, callbacks=[EarlyStopping(monitor='train_loss', mode = 'min')], default_root_dir=\"/data/tdesrous/qJetClassifier/logs\")\n",
    "trainer.fit(qClassifier, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tdesrous/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=39` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6fccf7ff504cf9807c5d38755f17ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160691/2837689712.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  edge_attr = nn.functional.softmax(self.l2(edge_attr))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss            1.623583119842475\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.623583119842475}]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(qClassifier, dataloaders=DataLoader(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d7272807367abe9a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d7272807367abe9a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
