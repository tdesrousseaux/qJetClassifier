{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Jet Classifier\n",
    "\n",
    "The aim of this project is to produce a jet classifier using a quantum graph neural network.\n",
    "\n",
    "This project is inspired of the following article: [*JEDI-net: a jet identification algorithm based on interaction networks*](https://arxiv.org/abs/1908.05318)\n",
    "\n",
    "This project was implemented on FPGAs with a simplified dataset (see [https://ipa.phys.ethz.ch/education/colloquium-ipa/current_semester.html](https://ipa.phys.ethz.ch/education/colloquium-ipa/current_semester.html)). This simplified version will be used as a reference for the current project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Circuit implementation\n",
    "\n",
    "The circuit takes a list of $N_p$ particles as an input. Each particle has $N_f$ features. \n",
    "\n",
    "A fully connected graph is first built the following way: \n",
    "- Each node corresponds to a particle and contains its features.\n",
    "- Each edge represents the concatenation of the features of the two corresponding nodes. \n",
    "\n",
    "A first circuit (for now a MLP) will transform the edges to a latent dimension (of dim $D_l$).\n",
    "\n",
    "The resulting graph structure is then embedded into a quantum circuit using angle encoding.\n",
    "\n",
    "The quantum circuit measurement gives the classification values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building the dataset\n",
    "\n",
    "The data come from https://zenodo.org/records/3602260\n",
    "\n",
    "It contains high-level features, but only restrained information are kept: for the N most energetic particles, $p_T, \\eta,\\phi$\n",
    "\n",
    "Then a graph structure is built using a fully connected graph from networkx. \n",
    "The nodes have the information of the particles. The edge represent the concatenation of the information of each pair of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/tdesrous/miniconda3/lib/python312.zip', '/data/tdesrous/miniconda3/lib/python3.12', '/data/tdesrous/miniconda3/lib/python3.12/lib-dynload', '', '/data/tdesrous/miniconda3/lib/python3.12/site-packages', '/data/tdesrous/qgraphs4feynman', '/data/tdesrous/qJetClassifier', '/data/tdesrous/qJetClassifier', '/data/tdesrous/qJetClassifier']\n"
     ]
    }
   ],
   "source": [
    "# Handles the data importing and small preprocessing for the interaction network.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import wget\n",
    "import tarfile\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "# import tensorflow as tf\n",
    "qJetClassifier_path = os.path.abspath(os.path.join('..', 'tdesrous/qJetClassifier'))\n",
    "sys.path.append(qJetClassifier_path)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from qJetClassifier import fit_standardisation, apply_standardisation\n",
    "# from fast_deepsets.data import plots\n",
    "\n",
    "class HLS4MLData150(object):\n",
    "    \"\"\"Data class for importing and processing the jet data.\n",
    "\n",
    "    The raw data is available at https://zenodo.org/records/3602260.\n",
    "    See Moreno et. al. 2019 - JEDI-net: a jet identification algorithm for a full\n",
    "    description of this data set, section 3.\n",
    "\n",
    "    Args:\n",
    "        root: The root directory of the data. It should contain a 'raw' and 'processed'\n",
    "            folder with raw and processed data. Otherwise, these will be generated.\n",
    "        nconst: The number of constituents the jet data should be sampled down to.\n",
    "            The raw number of constituents is 150.\n",
    "        feats: Which feature selection scheme should be applied. 'ptetaphi' for getting\n",
    "            the transverse momentum, pseudo-rapidity, and azimuthal angle of each\n",
    "            cosntituents for every jet. Otherwise, 'all' gets all the features of\n",
    "            each constituents.\n",
    "        norm: What kind of normalisation to apply to the features of the data.\n",
    "            Currently implemented: minmax, robust, or standard.\n",
    "        train: Whether to import the training data (True) or validation data (False)\n",
    "            of this data set.\n",
    "        seed: If provided, shuffles the *constituents* in the data set with given seed.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        nconst: int,\n",
    "        feats: str,\n",
    "        norm: str,\n",
    "        train: bool,\n",
    "        kfolds: 0,\n",
    "        seed: int = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root = Path(root)\n",
    "        self.nconst = nconst\n",
    "        self.norm = norm\n",
    "        self.feats = feats\n",
    "        self.train = train\n",
    "        self.type = \"train\" if self.train else \"val\"\n",
    "        self.seed = seed\n",
    "        self.min_pt = 2\n",
    "        self.kfolds = kfolds\n",
    "\n",
    "        self.train_url = (\n",
    "            \"https://zenodo.org/records/3602260/files/hls4ml_LHCjet_150p_train.tar.gz\"\n",
    "        )\n",
    "        self.test_url = (\n",
    "            \"https://zenodo.org/records/3602260/files/hls4ml_LHCjet_150p_val.tar.gz\"\n",
    "        )\n",
    "\n",
    "        self.preproc_output_name = f\"{self.type}_{self.nconst}const.npy\"\n",
    "        self.proc_output_name = (\n",
    "            f\"{self.type}_{self.norm}_{self.nconst}const_{self.feats}.npy\"\n",
    "        )\n",
    "        os.umask(0)\n",
    "        self.data_file_dir = self._get_raw_data()\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self._get_processed_data()\n",
    "        self._kfold()\n",
    "\n",
    "        self.njets = self.x.shape[0]\n",
    "        self.nfeats = self.x.shape[-1]\n",
    "\n",
    "    def _get_raw_data(self) -> str:\n",
    "        \"\"\"Downloads and unzips the raw data if it does not exist.\n",
    "\n",
    "        This method checks the given root directory specified the init of the class.\n",
    "        This root directory should have a specific structure, with the raw data files in\n",
    "        a subfolder called \"raw\".\n",
    "        \"\"\"\n",
    "        os.umask(0)\n",
    "        if not self._check_raw_data_exists():\n",
    "            self._download_data()\n",
    "\n",
    "        return self.root / \"raw\" / self.type\n",
    "\n",
    "    def _check_raw_data_exists(self) -> bool:\n",
    "        \"\"\"Checks if the data exists in the given root dir or needs to be downloaded.\"\"\"\n",
    "        os.umask(0)\n",
    "        if self.root.is_dir():\n",
    "            raw_dir = self.root / \"raw\"\n",
    "            if raw_dir.is_dir():\n",
    "                data_dir = raw_dir / self.type\n",
    "                if data_dir.is_dir():\n",
    "                    if any(data_dir.iterdir()):\n",
    "                        return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _check_preprocessed_data_exists(self) -> bool:\n",
    "        \"\"\"Checks if the preprocessed data exisits or needs to be re-processed.\"\"\"\n",
    "        if self.root.is_dir():\n",
    "            preproc_folder = self.root / \"preprocessed\"\n",
    "            x_preproc_file = preproc_folder / f\"x_{self.preproc_output_name}\"\n",
    "            y_preproc_file = preproc_folder / f\"y_{self.preproc_output_name}\"\n",
    "\n",
    "            if x_preproc_file.is_file() and y_preproc_file.is_file():\n",
    "                self.x_preprocessed = np.load(x_preproc_file)\n",
    "                self.y_preprocessed = np.load(y_preproc_file)\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _check_processed_data_exists(self) -> bool:\n",
    "        \"\"\"Checks if the processed data exisits or needs to be re-processed.\"\"\"\n",
    "        if self.root.is_dir():\n",
    "            proc_folder = self.root / \"processed\"\n",
    "            x_proc_file = proc_folder / f\"x_{self.proc_output_name}\"\n",
    "            y_proc_file = proc_folder / f\"y_{self.proc_output_name}\"\n",
    "\n",
    "            if x_proc_file.is_file() and y_proc_file.is_file():\n",
    "                self.x = np.load(x_proc_file)\n",
    "                self.y = np.load(y_proc_file)\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _download_data(self):\n",
    "        \"\"\"Downloads the jet data if it does not already exist.\"\"\"\n",
    "        raw_dir = self.root / \"raw\"\n",
    "        os.umask(0)\n",
    "        if not raw_dir.is_dir():\n",
    "            os.makedirs(raw_dir)\n",
    "\n",
    "        if self.train:\n",
    "            data_file_path = wget.download(self.train_url, out=str(raw_dir))\n",
    "        else:\n",
    "            data_file_path = wget.download(self.test_url, out=str(raw_dir))\n",
    "\n",
    "        print(\"\")\n",
    "        data_tar = tarfile.open(data_file_path, \"r:gz\")\n",
    "        data_tar.extractall(str(raw_dir))\n",
    "        data_tar.close()\n",
    "        os.remove(data_file_path)\n",
    "\n",
    "    def _import_raw_data(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Imports the raw data files into a numpy array.\"\"\"\n",
    "        dfiles = list(file for file in self.data_file_dir.iterdir() if file.is_file())\n",
    "        data = h5py.File(dfiles[0])\n",
    "        self.x_raw = data[\"jetConstituentList\"]\n",
    "        self.y_raw = data[\"jets\"][:, -6:-1]\n",
    "\n",
    "        for file_path in dfiles[1:]:\n",
    "            data = h5py.File(file_path)\n",
    "            add_x_data = data[\"jetConstituentList\"]\n",
    "            add_y_data = data[\"jets\"][:, -6:-1]\n",
    "            self.x_raw = np.concatenate((self.x_raw, add_x_data), axis=0)\n",
    "            self.y_raw = np.concatenate((self.y_raw, add_y_data), axis=0)\n",
    "\n",
    "    def _preproc_raw_data(self):\n",
    "        \"\"\"Applies preprocessing to the raw data.\n",
    "\n",
    "        The raw data contains jets (samples), each comprising up to 150 particle\n",
    "        constituents. Every constituent has a feature called transverse momentum which\n",
    "        describes, loosely speaking, the energy of the particle. This method filters\n",
    "        out all jet constituents that have this transverse momentum below a given\n",
    "        minimum threshold. Additionally, the constituents in the raw data are\n",
    "        ordered in descending order of tranverse momentum value. The first n\n",
    "        constituents are taken for each jet, where n is a number between 1 and 150.\n",
    "        \"\"\"\n",
    "        self.x_preprocessed = np.copy(self.x_raw)\n",
    "        del self.x_raw\n",
    "        self.y_preprocessed = np.copy(self.y_raw)\n",
    "        del self.y_raw\n",
    "        self._cut_transverse_momentum()\n",
    "        self._restrict_nb_constituents()\n",
    "\n",
    "        preproc_dir = self.root / \"preprocessed\"\n",
    "        if not preproc_dir.is_dir():\n",
    "            os.makedirs(preproc_dir)\n",
    "        np.save(preproc_dir / f\"x_{self.preproc_output_name}\", self.x_preprocessed)\n",
    "        np.save(preproc_dir / f\"y_{self.preproc_output_name}\", self.y_preprocessed)\n",
    "\n",
    "    def _process_data(self):\n",
    "        \"\"\"Processes the already processed data.\n",
    "\n",
    "        Namely, certain features are selected for each constituent of each jet.\n",
    "        Furthermore, each feature is normalized, using a certain normalization scheme.\n",
    "        For example, minmax normalization.\n",
    "        \"\"\"\n",
    "        self.x = np.copy(self.x_preprocessed)\n",
    "        del self.x_preprocessed\n",
    "        self.y = np.copy(self.y_preprocessed)\n",
    "        del self.y_preprocessed\n",
    "\n",
    "        proc_folder = self.root / \"processed\"\n",
    "        self._get_features()\n",
    "        norm_params = self._get_normalisation_params()\n",
    "\n",
    "        self.x = apply_standardisation(self.norm, self.x, norm_params)\n",
    "        if self.seed and self.train:\n",
    "            self.shuffle_constituents(self.seed)\n",
    "        self._plot_data()\n",
    "\n",
    "        if not proc_folder.is_dir():\n",
    "            os.makedirs(proc_folder)\n",
    "        proc_folder = self.root / \"processed\"\n",
    "        np.save(proc_folder / f\"x_{self.proc_output_name}\", self.x)\n",
    "        np.save(proc_folder / f\"y_{self.proc_output_name}\", self.y)\n",
    "\n",
    "        # Free up memory after finishing the preprocessing.\n",
    "        del self.x\n",
    "        del self.y\n",
    "        self.x = np.load(proc_folder / f\"x_{self.proc_output_name}\")\n",
    "        self.y = np.load(proc_folder / f\"y_{self.proc_output_name}\")\n",
    "\n",
    "    def _get_normalisation_params(self) -> list[float]:\n",
    "        \"\"\"Computes the normalisation parameters on the training data.\n",
    "\n",
    "        For example, computes the mean and standard deviation of a feature on the\n",
    "        training data and then applies it to the validation data.\n",
    "        This is done such that no information on the validation data is used before\n",
    "        passing it through the machine learning algorithm.\n",
    "        \"\"\"\n",
    "        proc_folder = self.root / \"processed\"\n",
    "        if not self.train:\n",
    "            train_data_name = proc_output_name = (\n",
    "                f\"train_{self.norm}_{self.nconst}const_{self.feats}.npy\"\n",
    "            )\n",
    "            try:\n",
    "                x_data_train = np.load(proc_folder / f\"x_{train_data_name}\")\n",
    "            except OSError as e:\n",
    "                print(\"Process training data with same hyperparameters first!\")\n",
    "                print(\"Need for normalisation of the validation data.\")\n",
    "                exit(1)\n",
    "\n",
    "            return fit_standardisation(self.norm, self.x)\n",
    "\n",
    "        return fit_standardisation(self.norm, self.x)\n",
    "\n",
    "    def _get_processed_data(self):\n",
    "        \"\"\"Imports the processed data if it exists. If not, generates it.\"\"\"\n",
    "        if not self._check_processed_data_exists():\n",
    "            if not self._check_preprocessed_data_exists():\n",
    "                self._import_raw_data()\n",
    "                self._preproc_raw_data()\n",
    "\n",
    "            self._process_data()\n",
    "\n",
    "    def _plot_data(self):\n",
    "        \"\"\"Plots the normalised data.\"\"\"\n",
    "        print(\"Plotting data...\")\n",
    "        # plots_folder = self.root / f\"plots_{self.norm}_{self.nconst}const_{self.feats}\"\n",
    "        # if not plots_folder.is_dir():\n",
    "        #     os.makedirs(plots_folder)\n",
    "\n",
    "        # plots.constituent_number(plots_folder, self.x, self.type)\n",
    "        # plots.normalised_data(plots_folder, self.x, self.y, self.type, self.feats)\n",
    "\n",
    "    def _get_features(self) -> np.ndarray:\n",
    "        \"\"\"Choose what feature selection to employ on the data. Return shape.\"\"\"\n",
    "        switcher = {\n",
    "            \"ptetaphi\": lambda: self._select_features_ptetaphi(self.x),\n",
    "            \"allfeats\": lambda: self._select_features_all(self.x),\n",
    "        }\n",
    "\n",
    "        self.x = switcher.get(self.feats, lambda: None)()\n",
    "        if self.x is None:\n",
    "            raise TypeError(\"Feature selection name not valid!\")\n",
    "\n",
    "    def _select_features_ptetaphi(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Selects (pT, etarel, phirel) features from the numpy jet array.\"\"\"\n",
    "        return data[:, :, [5, 8, 11]]\n",
    "\n",
    "    def _select_features_all(self, data: np.ndarray):\n",
    "        \"\"\"Gets all the features from the numpy jet array.\n",
    "\n",
    "        The features in this kind of 'selection' are:'\n",
    "        (px, py, pz, E, Erel, pT, ptrel, eta, etarel, etarot, phi, phirel, phirot,\n",
    "        deltaR, cos(theta), cos(thetarel), pdgid)\n",
    "        \"\"\"\n",
    "        return data[:, :, :]\n",
    "\n",
    "    def _cut_transverse_momentum(self):\n",
    "        \"\"\"Remove constituents that are below a certain transverse momentum from jets.\n",
    "\n",
    "        If a jet has no constituents with a momentum above the given threshold, then\n",
    "        the whole jet is removed.\n",
    "        \"\"\"\n",
    "        boolean_mask = self.x_preprocessed[:, :, 5] > self.min_pt\n",
    "        structure_memory = boolean_mask.sum(axis=1)\n",
    "        self.x_preprocessed = np.split(\n",
    "            self.x_preprocessed[boolean_mask, :], np.cumsum(structure_memory)[:-1]\n",
    "        )\n",
    "        self.x_preprocessed = [\n",
    "            jet_const for jet_const in self.x_preprocessed if jet_const.size > 0\n",
    "        ]\n",
    "        self.y_preprocessed = self.y_preprocessed[structure_memory > 0]\n",
    "\n",
    "    def _restrict_nb_constituents(self) -> np.ndarray:\n",
    "        \"\"\"Force each jet to have an equal number of constituents.\n",
    "\n",
    "        If the jet has more constituents then the given number, the surplus is discarded.\n",
    "        If the jet has less than the given number, then the jet vector is padded with 0\n",
    "        values until its length reaches the given number.\n",
    "        \"\"\"\n",
    "        for jet in range(len(self.x_preprocessed)):\n",
    "            if self.x_preprocessed[jet].shape[0] >= self.nconst:\n",
    "                self.x_preprocessed[jet] = self.x_preprocessed[jet][: self.nconst, :]\n",
    "            else:\n",
    "                padding_length = self.nconst - self.x_preprocessed[jet].shape[0]\n",
    "                self.x_preprocessed[jet] = np.pad(\n",
    "                    self.x_preprocessed[jet], ((0, padding_length), (0, 0))\n",
    "                )\n",
    "        self.x_preprocessed = np.array(self.x_preprocessed)\n",
    "\n",
    "    def shuffle_constituents(self, seed: int):\n",
    "        \"\"\"Shuffles the constituents based on an array of seeds.\n",
    "\n",
    "        Each jet's constituents is shuffled with respect to a seed that is fixed.\n",
    "        This seed is different for each jet.\n",
    "        \"\"\"\n",
    "        print(\"Shuffling constituents...\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "        seeds = rng.integers(low=0, high=10000, size=self.x.shape[0])\n",
    "\n",
    "        for jet_idx, seed in enumerate(seeds):\n",
    "            shuffling = np.random.RandomState(seed=seed).permutation(self.x.shape[1])\n",
    "            self.x[jet_idx, :] = self.x[jet_idx, shuffling]\n",
    "\n",
    "        print(f\"Shuffling done!\")\n",
    "\n",
    "    def _kfold(self):\n",
    "        \"\"\"Creates a kfolded view of the data.\"\"\"\n",
    "        if self.kfolds <= 0 or not self.train:\n",
    "            return\n",
    "\n",
    "        # print(tcols.OKGREEN)\n",
    "        print(f\"Splitting the data into k={self.kfolds} kfolds.\")\n",
    "        # print(tcols.ENDC)\n",
    "\n",
    "        kfolder = sklearn.model_selection.StratifiedKFold(\n",
    "            n_splits=self.kfolds, shuffle=True\n",
    "        )\n",
    "        # Convert back from one-hot to class targets since sklearn function does not\n",
    "        # like one-hot targets.\n",
    "        self.kfolds = kfolder.split(self.x, np.argmax(self.y, axis=-1))\n",
    "\n",
    "    def show_details(self):\n",
    "        \"\"\"Prints some key details of the data set.\"\"\"\n",
    "        data_type = \"Training\" if self.train else \"Validation\"\n",
    "        print(f\"{data_type} data details:\")\n",
    "        print(f\"Dataset size: {self.x.shape[0]} jets\")\n",
    "        print(f\"Number of constituents: {self.x.shape[1]}\")\n",
    "        print(f\"Number of features: {self.x.shape[2]}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting standard normalisation...\n",
      "The normalisation parameters are: \n",
      "x_mean: [106.84654357547497, 5.641138303586946e-06, -1.3848154403908808e-05]\n",
      "x_std: [90.8040073747704, 0.05967986854536945, 0.05971903042966309]\n",
      "-------------------------\n",
      "\n",
      "Shuffling constituents...\n",
      "Shuffling done!\n",
      "Plotting data...\n"
     ]
    }
   ],
   "source": [
    "dataset = HLS4MLData150(\n",
    "    root=\"/data/tdesrous/qJetClassifier/data\",\n",
    "    nconst=6,\n",
    "    feats=\"ptetaphi\",\n",
    "    norm=\"standard\",\n",
    "    train=True,\n",
    "    kfolds=0,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetDataset(Dataset):\n",
    "    def __init__(self, file_path: str, n_elements=750):\n",
    "        self.dataset = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __load_data__(self, file_path: str):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quantum circuit\n",
    "\n",
    "The quantum circuit takes as an input a fully connected graph.\n",
    "\n",
    "It uses data uploading the following way:\n",
    "- The node information are encoded using $R_X$, $R_Y$ and $R_Z$ rotations gates (there are three parameters by node). (Other possibility: use only $R_X$ gates with a vector encoding)\n",
    "- The edge information are encoded using $R_{ZZ}$ rotation gates.\n",
    "\n",
    "All the gates are parametrized, using equivariant layers (regarding the node permutations). It means that the trainable parameters are the same for each layer. \n",
    "\n",
    "Then a equivariant ansatz is applied.\n",
    "\n",
    "This process is done $N_R$ times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_X_layer(graph, parameters):\n",
    "    \"\"\"Encode the nodes of the graph using R_X gates, multiplied by trainable parameters\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        parameters (torch.Tensor): the trainable parameters for the equivariant layer\n",
    "    \"\"\"\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        qml.RX(np.inner(parameters, graph.nodes[node]['features']),\n",
    "               wires=node)\n",
    "        \n",
    "def ZZ_layer(graph, parameters):\n",
    "    \"\"\"Encode the edges of the graph using ZZ gates, multiplied by trainable parameters\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        parameters (torch.Tensor): the trainable parameters for the equivariant layer\n",
    "    \"\"\"\n",
    "\n",
    "    for edge in graph.edges:  #The order doesn't matter, the gates commute between each other\n",
    "        qml.IsingZZ(np.inner(parameters, graph.edges[edge]['message']),\n",
    "                    wires=edge[:2])\n",
    "\n",
    "\n",
    "def feature_map(graph, node_parameters, edge_parameters):\n",
    "    \"\"\"Encode the nodes and edges of the graph using R_X and ZZ gates, multiplied by trainable parameters\n",
    "\n",
    "    Args:\n",
    "        graph (Networkx.Graph): a fully connected graph representing the jet\n",
    "        node_parameters (torch.Tensor): the trainable parameters for the nodes\n",
    "        edge_parameters (torch.Tensor): the trainable parameters for the edges\n",
    "    \"\"\"\n",
    "\n",
    "    R_X_layer(graph, node_parameters)\n",
    "    ZZ_layer(graph, edge_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
